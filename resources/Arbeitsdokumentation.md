Arbeitsdokumentation
====================


Crawler
-------

### Funktionen ###
In einer frühen Version arbeitete der Crawler in einem Modus, der aus einer Liste von Webseiten jeweils nur die Landing Page erfasst und in die Datenbank übernimmt. Alle anderen Webseiten wurden ignoriert.
Während der Entwicklung wurde ein zweiter Modus hinzugefügt, der Webseiten und ihre Unterseiten erfasst, damit mehr Seiten mit potentiell hoher Unusability gesammelt werden können. Weiterhin werden nun auch sämtliche Links auf bisher gefundenen Seiten weiter verfolgt (rekursiver Crawl).

### Probleme & Lösungen ###

### Ausblick ###

- Größere Datenbasis erfassen
- Ranking-Faktoren (Unusability) während des Crawlens bestimmen


Indexer
-------

NLP
---


Interface
---------

Ranking
-------

Datenbank
---------

Projektmanagement
-----------------